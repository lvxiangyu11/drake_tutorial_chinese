{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4dcbebc69eab41b386149fa4ced7df8e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# 自动微分基础\n",
    "关于如何运行本教程的 Jupyter 笔记本，请参见 [index](./index.ipynb)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d39ebc2738c5401da70654ae6d91b613",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "本笔记本简要介绍了 Drake 中的自动微分（Automatic Differentiation, AutoDiff）。内容包括：\n",
    "- 基本的 Eigen/Numpy 运算的自动微分，\n",
    "- 动力学系统（如状态更新、输出计算等）的自动微分，\n",
    "- 仿真采样数据的自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c36619a246d647848673a590f845328b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Eigen/Numpy 运算\n",
    "在 Drake 中，自动微分基于 Eigen 的 AutoDiffScalar 实现。因此，任何显式的 Eigen（在 Python 中即 Numpy）运算都可以自动求导。我们来看一个简单例子 $\\mathbf{y} = \\mathbf{a}^{\\top} \\mathbf{x} = [1,~ 2] \\mathbf{x}$。我们希望通过自动微分获得 $\\partial \\mathbf{y} / \\partial \\mathbf{x} = \\mathbf{a}^{\\top}$。实现方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "189fc711326b41ffabf229123b1f553d",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 27,
    "execution_start": 1755522588095,
    "source_hash": "9ebd70d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double type array:\n",
      " [[0.95825836]\n",
      " [0.60175035]]\n",
      "converted to AutoDiffXd scalar type array:\n",
      " [[<AutoDiffXd 0.9582583588349217 nderiv=2>]\n",
      " [<AutoDiffXd 0.6017503463561198 nderiv=2>]]\n",
      "Gradient: [[1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydrake.autodiffutils import InitializeAutoDiff, ExtractGradient\n",
    "\n",
    "a = np.array([1, 2]).reshape([2, -1])\n",
    "x = np.random.rand(2, 1)\n",
    "print(\"double 类型数组:\\n\", x)\n",
    "x = InitializeAutoDiff(x)\n",
    "print(\"转换为 AutoDiffXd 标量类型数组:\\n\", x)\n",
    "y = a.T @ x\n",
    "dydx = ExtractGradient(y)\n",
    "print(\"梯度:\", ExtractGradient(y))\n",
    "np.testing.assert_allclose(a.T, dydx)  # 验证 dy/dx = a^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d3c24a2308ed44c4a2bc7a384d07201b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "注意，自动微分只需在常规 Numpy 运算基础上增加两步：1）用 `InitializeAutoDiff(x)` 声明需要对其求导的变量，2）最后用 `ExtractGradient(y)` 提取梯度。\n",
    "\n",
    "我们也可以对多个变量同时求导。例如 $\\mathbf{y} = \\mathbf{a}_1^{\\top} \\mathbf{x}_1 + \\mathbf{a}_2^{\\top} \\mathbf{x}_2$，我们希望分别获得 $\\partial \\mathbf{y} / \\partial \\mathbf{x}_1$ 和 $\\partial \\mathbf{y} / \\partial \\mathbf{x}_2$。实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "eb06a9988a3942829fd42afcaf40fb00",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522588175,
    "source_hash": "e0b153af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All gradients: [[1. 2. 1. 2. 3.]]\n",
      "Gradients calculated by automatic differentiation:\n",
      "a1^T = [1. 2.]\n",
      "a2^T = [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydrake.autodiffutils import InitializeAutoDiffTuple, ExtractGradient\n",
    "\n",
    "a1 = np.array([1, 2]).reshape([2, -1])\n",
    "x1 = np.random.rand(2, 1)\n",
    "\n",
    "a2 = np.array([1, 2, 3]).reshape([3, -1])\n",
    "x2 = np.random.rand(3, 1)\n",
    "\n",
    "x1, x2 = InitializeAutoDiffTuple(x1, x2)\n",
    "y = a1.T @ x1+ a2.T @ x2\n",
    "\n",
    "dydx = ExtractGradient(y)\n",
    "print(\"所有梯度:\", dydx)\n",
    "dydx1 = dydx[0][0:2]\n",
    "dydx2 = dydx[0][2:]\n",
    "print(\"自动微分计算的梯度:\")\n",
    "print(\"a1^T =\", dydx1)\n",
    "np.testing.assert_allclose(a1.T, [dydx1])\n",
    "print(\"a2^T =\", dydx2)\n",
    "np.testing.assert_allclose(a2.T, [dydx2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da01ca17fdba428dbcc7b4bfed527097",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "注意，`ExtractGradient(y)` 会提取 `y` 对所有通过 `InitializeAutodiffTuple` 声明的变量的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c0f8e3fc038f4f308022b7d0e1c0ce14",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Dynamical Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1562faf4f16a4256857e24acce66fbcd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Drake's Built-in Dynamical Systems\n",
    "\n",
    "Drake's most built-in systems' dynamics only involve explicit Eigen operations. Hence, they are all automatically differentiable. Let's consider the simple discrete-time [LinearSystem](https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1_linear_system.html), whose dynamics is given as\n",
    "$$x_{t+1} = x_t +  2u_t, ~~~y_{t} = 3x_t + 4u_t.$$\n",
    "For general dynamical systems, the derivatives of next state w.r.t. state $\\partial x_{t+1}/ \\partial x_t$ and input $\\partial x_{t+1}/ \\partial u_t$, and output w.r.t. state $\\partial y_{t}/ \\partial x_t$ and input $\\partial y_{t}/ \\partial u_t$ are frequently wanted. Here we will show how to obtain them via automatic differentiation. Let's first construct the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "7bc662d4a7d541919647204774b205c4",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 47,
    "execution_start": 1755522588225,
    "source_hash": "a359fa45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A system using double: <pydrake.systems.primitives.LinearSystem object at 0x7ffb576bcd70>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydrake.systems.primitives import LinearSystem\n",
    "from pydrake.autodiffutils import InitializeAutoDiffTuple, ExtractGradient\n",
    "\n",
    "A = np.array([[1]])\n",
    "B = np.array([[2]])\n",
    "C = np.array([[3]])\n",
    "D = np.array([[4]])\n",
    "timestep = 1  # so that the system is discrete-time\n",
    "system = LinearSystem(A, B, C, D, timestep)\n",
    "\n",
    "print(\"A system using double:\", system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab26fbe0473b4cdeaab267a89d5d4e1a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "By default, the system uses `double` as the scalar type. We need to convert it to use [drake::AutoDiffXd](https://drake.mit.edu/doxygen_cxx/namespacedrake.html#a35725b277b02aeb79f24fd7f724e6dbc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "af6fffc4e14344bc980f3d3c0d3d6ae0",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522588335,
    "source_hash": "bf2edcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system converted to AutoDiffXd: <pydrake.systems.primitives.LinearSystem_𝓣AutoDiffXd𝓤 object at 0x7ffb576bd0d0>\n"
     ]
    }
   ],
   "source": [
    "system_ad = system.ToAutoDiffXd()\n",
    "print(\"The system converted to AutoDiffXd:\", system_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "39851a6d55944ec99edb2447e36c004a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let's set $x_t = 1$ and $u_t=1$ (or any real numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "395fd4f43071471ebcd82e3245c2e45a",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588386,
    "source_hash": "1634c8bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pydrake.systems.framework.FixedInputPortValue at 0x7ffb61dc8b30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ad = system_ad.CreateDefaultContext()\n",
    "x = np.array([1])\n",
    "u = np.array([1])\n",
    "x, u = InitializeAutoDiffTuple(x, u)\n",
    "context_ad.SetDiscreteState(0, x)\n",
    "system_ad.get_input_port(0).FixValue(context_ad, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4ef499ffd8c84425bb91756b54cddcdf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Then, we calculate the derivatives of next states $\\partial x_{t+1}/ \\partial x_t=1$ and $\\partial x_{t+1}/ \\partial u_t=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "67646a1f58cf457cad0134d49207e8ed",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522588435,
    "source_hash": "28d268d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients calculated by automatic differentiation:\n",
      "dx'/dx = 1.0\n",
      "dx'/du = 2.0\n"
     ]
    }
   ],
   "source": [
    "# allocate the state object\n",
    "x_next_object = system_ad.AllocateContext().get_discrete_state()  \n",
    "# store value to x_next_object without modifying context\n",
    "system_ad.CalcForcedDiscreteVariableUpdate(context_ad, x_next_object)  \n",
    "# to extract numpy array from the state object\n",
    "x_next = x_next_object.get_vector(0).CopyToVector()  \n",
    "grad = ExtractGradient(x_next)\n",
    "dx_next_dx = grad.flatten()[0]\n",
    "dx_next_du = grad.flatten()[1]\n",
    "\n",
    "print(\"Gradients calculated by automatic differentiation:\")\n",
    "print(\"dx'/dx =\", dx_next_dx)\n",
    "assert dx_next_dx == 1\n",
    "print(\"dx'/du =\", dx_next_du)\n",
    "assert dx_next_du == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cf72841ef9264756b410f6a3b07ce248",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "and the derivatives of output $\\partial y_{t}/ \\partial x_t=3$ and $\\partial x_{t}/ \\partial u_t=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "f5fee2c094094da4b7956291717a5868",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522588485,
    "source_hash": "ba473e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients calculated by automatic differentiation::\n",
      "dy/dx = 3.0\n",
      "dy/du = 4.0\n"
     ]
    }
   ],
   "source": [
    "output_object = system_ad.AllocateOutput()\n",
    "system_ad.CalcOutput(context_ad, output_object)\n",
    "output_port_index = system_ad.get_output_port(0).get_index()\n",
    "output = output_object.get_vector_data(output_port_index).CopyToVector()\n",
    "grad = ExtractGradient(output)\n",
    "dy_dx = grad.flatten()[0]\n",
    "dy_du = grad.flatten()[1]\n",
    "print(\"Gradients calculated by automatic differentiation::\")\n",
    "print(\"dy/dx =\", dy_dx)\n",
    "assert dy_dx == 3\n",
    "print(\"dy/du =\", dy_du)\n",
    "assert dy_du == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b8c837a58fe7493a924e6bbd404bf4d0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Write Your Own Dynamical Systems for Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4e96d91cc485459aaab2b8f64e8d0618",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "You can write your own [LeafSystem](https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1_leaf_system.html) that supports automatic differentiation by using the [drake::AutoDiffXd](https://drake.mit.edu/doxygen_cxx/namespacedrake.html#a35725b277b02aeb79f24fd7f724e6dbc) scalar type as the template value. In Python, you can do so using the [TemplateSystem](https://drake.mit.edu/pydrake/pydrake.systems.scalar_conversion.html) utility. Let's consider the simple discrete-time system \n",
    "$$x_{t+1} = x_t +  2u_t, ~~~y_{t} = x^2_t.$$\n",
    "We will build it with a discrete-time [LinearSystem](https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1_linear_system.html), and a custom system that squares the input as the output. Let's first define the linear system $x_{t+1} = x_t +  2u_t, ~~~y_{t} = x_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "789181111ca94dd4ba64e140e4fcb9e3",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588535,
    "source_hash": "78d10a4e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydrake.systems.scalar_conversion import TemplateSystem\n",
    "from pydrake.systems.primitives import LinearSystem\n",
    "from pydrake.systems.framework import (\n",
    "    BasicVector_,\n",
    "    DiagramBuilder,\n",
    "    LeafSystem_,\n",
    ")\n",
    "from pydrake.autodiffutils import InitializeAutoDiffTuple, ExtractGradient\n",
    "\n",
    "A = np.array([[1]])\n",
    "B = np.array([[2]])\n",
    "C = np.array([[1]])\n",
    "D = np.array([[0]])\n",
    "timestep = 1\n",
    "linear_system = LinearSystem(A, B, C, D, timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db13ed87db6b49fcbef065f3ac009eec",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now, let's define the templated custom system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "3f2a78b901ed4226af3ab171d1467baa",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 103,
    "execution_start": 1755522588585,
    "source_hash": "2c87ea7a"
   },
   "outputs": [],
   "source": [
    "@TemplateSystem.define(\"SquareSystem_\")\n",
    "def SquareSystem_(T):\n",
    "    class Impl(LeafSystem_[T]):\n",
    "        def _construct(self, dimension: int, converter=None):\n",
    "            LeafSystem_[T].__init__(self, converter=converter)\n",
    "            self.dimension = dimension\n",
    "            self.input_port = self.DeclareVectorInputPort(\n",
    "                \"input\", BasicVector_[T](dimension)\n",
    "            )\n",
    "            self.output_port = self.DeclareVectorOutputPort(\n",
    "                \"output\",\n",
    "                BasicVector_[T](dimension),\n",
    "                self.calc_output,\n",
    "            )\n",
    "\n",
    "        def _construct_copy(self, other, converter=None):\n",
    "            Impl._construct(self, other.dimension, converter=converter)\n",
    "\n",
    "        def calc_output(self, context, output):\n",
    "            input_array = self.input_port.Eval(context)\n",
    "            # Element-wise squared input as the output y = x * x\n",
    "            output.set_value(input_array * input_array)\n",
    "\n",
    "    return Impl\n",
    "\n",
    "SquareSystem = SquareSystem_[None]  # The default system that uses double as the scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "195c04d1d1a44af3b40eee359aad9dbc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The main difference from what we saw in [Modeling Dynamical Systems](./dynamical_systems.ipynb) is the use of template classes `LeafSystem_[T]` and `BasicVector_[T]`. The default `double`-scalar class, defined as `SquareSystem = SquareSystem_[None]`, is nearly identical to a version defined without using template classes. However, by templating the system, Drake can automatically convert it to use [drake::AutoDiffXd](https://drake.mit.edu/doxygen_cxx/namespacedrake.html#a35725b277b02aeb79f24fd7f724e6dbc) for automatic differentiation. Now let’s construct the custom system and compose the full diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "94d127ec85f34f97af7746f8f7a611ef",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588735,
    "source_hash": "e29fd421"
   },
   "outputs": [],
   "source": [
    "squared_output = SquareSystem(dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "89f1883a0da74959abe524e75d27bd65",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522588795,
    "source_hash": "ada60f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default double systems:\n",
      " [<pydrake.systems.primitives.LinearSystem object at 0x7ffb576bcd10>, <pydrake.systems.scalar_conversion.SquareSystem_𝓣float𝓤 object at 0x7ffb53cea9f0>]\n"
     ]
    }
   ],
   "source": [
    "builder = DiagramBuilder()\n",
    "builder.AddSystem(linear_system)\n",
    "builder.AddSystem(squared_output)\n",
    "builder.Connect(linear_system.get_output_port(0), squared_output.get_input_port(0))\n",
    "builder.ExportInput(linear_system.get_input_port(), \"input\")\n",
    "builder.ExportOutput(squared_output.get_output_port(), \"output\")\n",
    "# The full dynamical system we are considering\n",
    "system = builder.Build()  \n",
    "print(\"Default double systems:\\n\", system.GetSystems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5e59a7b3621d48c48b6fd6db8cee42b9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Note that although we only construct the default `SquareSystem` that uses `double` as the scalar, it can be converted into a system using [AutoDiffXd](https://drake.mit.edu/doxygen_cxx/namespacedrake.html#a35725b277b02aeb79f24fd7f724e6dbc) as the scalar when we do `ToAutoDiffXd()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "d76918c1ada04104be63d7c093b5e7a2",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588845,
    "source_hash": "3d5e20f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiffXd systems:\n",
      " [<pydrake.systems.primitives.LinearSystem_𝓣AutoDiffXd𝓤 object at 0x7ffb53ceaf30>, <pydrake.systems.scalar_conversion.SquareSystem_𝓣AutoDiffXd𝓤 object at 0x7ffb53cead50>]\n"
     ]
    }
   ],
   "source": [
    "system_ad = system.ToAutoDiffXd()\n",
    "print(\"AutoDiffXd systems:\\n\", system_ad.GetSystems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c7a3d4489601450da1e7a040a6b4e09f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now let's calculate the derivatives at $x_t=1$ and $u_t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "36c377914d2f4819969b291b5f87cce9",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588895,
    "source_hash": "1634c8bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pydrake.systems.framework.FixedInputPortValue at 0x7ffb61d490f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ad = system_ad.CreateDefaultContext()\n",
    "x = np.array([1])\n",
    "u = np.array([1])\n",
    "x, u = InitializeAutoDiffTuple(x, u)\n",
    "context_ad.SetDiscreteState(0, x)\n",
    "system_ad.get_input_port(0).FixValue(context_ad, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c488807edc4d4be69aa5252cbde7d210",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We obtain the correct derivatives $\\left. \\partial y_{t}/ \\partial x_t \\right|_{x_t=1}= \\left. 2 x_t \\right|_{x_t=1} = 2$ and $\\partial y_{t}/ \\partial u_t=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "ec86b9a95db944c9b7673a99f3dab7a2",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588945,
    "source_hash": "a3951c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients calculated by automatic differentiation:\n",
      "dy/dx = 2.0\n",
      "dy/du = 0.0\n"
     ]
    }
   ],
   "source": [
    "output_object = system_ad.AllocateOutput()\n",
    "system_ad.CalcOutput(context_ad, output_object)\n",
    "output_port_index = system_ad.GetOutputPort(\"output\").get_index()\n",
    "output = output_object.get_vector_data(output_port_index).CopyToVector()\n",
    "grad = ExtractGradient(output)\n",
    "dy_dx = grad.flatten()[0]\n",
    "dy_du = grad.flatten()[1]\n",
    "print(\"Gradients calculated by automatic differentiation:\")\n",
    "print(\"dy/dx =\", dy_dx)\n",
    "assert dy_dx == 2\n",
    "print(\"dy/du =\", dy_du)\n",
    "assert dy_du == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f807b8a2f7fa48caa83db19e7cc4f17f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Simulation Rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b40cdc2d6f64197bf10fb162c8e0b79",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Lastly, let's simulate a continuous-time system, and differentiate the sampled simulation results. We consider the linear system\n",
    "$$ \\dot{x} = -x,~~ y = x.$$\n",
    "Given the initial state $x_0$, its output solution is given as\n",
    "$$ y(t) = e^{-t} x_0,$$\n",
    "and the output's derivative w.r.t. the initial state is given as \n",
    "$$ \\frac{\\partial y}{\\partial x_0} = e^{-t},$$\n",
    "which only depends on time. Now we will calculate this derivative through automatic differentiation, and compare the results to the analytical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "a9f5e47b004442fd952aa8dc900ef489",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522588995,
    "source_hash": "39ea430"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_analytical_gradient(t):\n",
    "    return np.exp(-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3ab239b867aa4041aff911c574e21526",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let's construct the linear system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "81e11e2fadd0466cbb786bdd62e6598f",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522589045,
    "source_hash": "2c97a94a"
   },
   "outputs": [],
   "source": [
    "from pydrake.systems.primitives import LinearSystem, LogVectorOutput\n",
    "from pydrake.systems.framework import DiagramBuilder\n",
    "from pydrake.autodiffutils import InitializeAutoDiff, ExtractGradient, AutoDiffXd\n",
    "from pydrake.systems.analysis import Simulator_\n",
    "\n",
    "A = np.array([[-1]])\n",
    "B = np.array([[0]])\n",
    "C = np.array([[1]])\n",
    "D = np.array([[0]])\n",
    "timestep = 0  # so that the system is continuous-time\n",
    "linear_system = LinearSystem(A, B, C, D, timestep)\n",
    "\n",
    "builder = DiagramBuilder()\n",
    "builder.AddSystem(linear_system)\n",
    "builder.ExportInput(linear_system.get_input_port(), \"input\")\n",
    "builder.ExportOutput(linear_system.get_output_port(), \"output\")\n",
    "logger = LogVectorOutput(linear_system.get_output_port(), builder, publish_period=0.1)\n",
    "system = builder.Build()  # The dynamical system we are considering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "eddaaa7ff8ab42ce8904d85dbf22b2f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "and convert it to use `AutoDiffXd` scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "8b786f4eed4b424382aeb97b3fd6ab88",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522589095,
    "source_hash": "99f3ad0d"
   },
   "outputs": [],
   "source": [
    "system_ad = system.ToAutoDiffXd()\n",
    "logger_ad = system_ad.GetSystems()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e0a5d648514c441d81b04fea9edd0f95",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We get the `AutoDiffXd` version of the logger to extract the simulation results later. Now we construct a `Simulator` that uses `AutoDiffXd` scalar, and set an arbitrary initial state $x_0 = 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "febe98d5d8ee4b40aa1f627b6f4f1bc8",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 1,
    "execution_start": 1755522589145,
    "source_hash": "f18b838d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pydrake.systems.analysis.SimulatorStatus at 0x7ffb53ce7ab0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator_ad = Simulator_[AutoDiffXd](system_ad)\n",
    "context_ad = simulator_ad.get_mutable_context()\n",
    "system_ad.get_input_port(0).FixValue(context_ad, 0)\n",
    "x0 = np.array([5])\n",
    "x0 = InitializeAutoDiff(x0)\n",
    "context_ad.SetContinuousState(x0)\n",
    "simulator_ad.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "05ec474e8ca042cfb9e6f79e1d24f035",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Finally, we simulate for 1 second, and assert that the derivatives calculated by automatic differentiation match the analytical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "5a11fa2aa29a4ea99bfab7642e47f278",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522589195,
    "source_hash": "72ea5333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0 at t = 0.30000000000000004\n",
      "Analytical gradient: 0.7408182206817179\n",
      "Gradient calculated by autodiff: 0.7408182212987496\n"
     ]
    }
   ],
   "source": [
    "simulator_ad.AdvanceTo(1)\n",
    "log = logger_ad.FindLog(simulator_ad.get_context())\n",
    "# convert AutoDiffXd back to double\n",
    "sample_times = np.array([t.value() for t in log.sample_times()])\n",
    "# calcualte gradients analytically and via autodiff\n",
    "analytical_gradients = calculate_analytical_gradient(sample_times)\n",
    "autodiff_gradients = ExtractGradient(log.data()).flatten()\n",
    "# Let's print the data at an arbitrary sample time\n",
    "sample_index = 3\n",
    "print(\"dy/dx0 at t =\", sample_times[sample_index]),\n",
    "print(\"Analytical gradient:\", analytical_gradients[sample_index])\n",
    "print(\"Gradient calculated by autodiff:\", autodiff_gradients[sample_index])\n",
    "# We assert that the autodiff gradients are correct at all sample times\n",
    "np.testing.assert_allclose(autodiff_gradients, analytical_gradients, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b9f88f7efd764110a9540d0ea01f8b0b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "**System Scalar Types and Conversions in Drake**  \n",
    "- [Default Scalars](https://drake.mit.edu/doxygen_cxx/group__default__scalars.html): Overview of the scalar types commonly used in Drake, such as `double`, `AutoDiffXd`, and `symbolic::Expression`.  \n",
    "- [System Scalar Conversion](https://drake.mit.edu/doxygen_cxx/group__system__scalar__conversion.html): Describes how Drake systems support conversions between scalar types to enable features like automatic differentiation and symbolic analysis.\n",
    "\n",
    "**Automatic Differentiation with Eigen**  \n",
    "- [An Introduction to Automatic Differentiation in Eigen (PDF)](https://github.com/edrumwri/drake/blob/bbc944fec87f7dac13169c65c961db29906435fb/drake/doc/autodiff_intro/autodiff.pdf)\n",
    "\n",
    "**Automatic Differentiation with Drake’s Hydroelastic Contact Model**  \n",
    "- *Kurtz, V., & Lin, H.* (2022). Contact-Implicit Trajectory Optimization with Hydroelastic Contact and iLQR. *IEEE/RSJ IROS 2022*. [link](https://ieeexplore.ieee.org/abstract/document/9981686) [code](https://github.com/vincekurtz/drake_ddp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "afa4c6aadde841e6aea144edd69c8141",
    "deepnote_cell_type": "code",
    "execution_context_id": "ccf23b0b-8521-4ca2-8222-1e214aa413d8",
    "execution_millis": 0,
    "execution_start": 1755522589245,
    "source_hash": "b623e53d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "dd0b74cdebaa4fdda45591d9b69edf06",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": "4",
 "nbformat_minor": "0"
}
